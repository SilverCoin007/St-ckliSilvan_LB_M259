{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Bestimmen Sie, welche Felder Ihrer Daten für Ihr Modell besonders aussagekräftig sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Spalte \"high\", ist die Spalte \"volume\" am wichtigsten. Denn alleine die anderen Spalten sind sonst nutzlos. Mit der Spalte \"volume\" allerdings kann dann eine sinnvolle Berechnung durchgeführt werden, wobei die \"nutzlosen Spalten\" mit der Spalte \"volume\" verrechnet werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Wählen Sie eine geeignete Messmetrik für Ihr Modell und berechnen Sie sie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "bitcoin = os.path.join(\"Datenset_LB_M259 - Demo.csv\")\n",
    "Bitcoin = pd.read_csv(bitcoin)\n",
    "Bitcoin.head()\n",
    "\n",
    "# x = features\n",
    "X = Bitcoin[[\"low\", \"open\", \"close\", \"volume\"]]\n",
    "\n",
    "# y = targets\n",
    "y = Bitcoin[[\"high\"]]\n",
    "X.head()\n",
    "\n",
    "y=y.astype(\"int\")\n",
    "# Dieser Codeabschnitt gibt die Anzahl zeilen zurück die NaN enthalten.\n",
    "len(X[X.isna().any(axis=1)])\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "si = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "si.fit(X)\n",
    "X_ = si.transform(X)\n",
    "X = pd.DataFrame(X_, columns=X.columns)\n",
    "len(X[X.isna().any(axis=1)])\n",
    "# Da die Ausgabe gleich Null ist, muss ich nichts weiteres unternehmen.\n",
    "# Ausserdem habe ich keine NaN Felder. Deshalb muss ich auch da nichts unternehmen.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   X, y, test_size = 0.2, random_state = 42\n",
    ")\n",
    "len(X_train), len(X_test)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "algorithms = {\n",
    "    # \"Nearest Neighbors\" : KNeighborsClassifier(3),\n",
    "    # \"Stochastic Gradient Descent\" : SGDClassifier(),\n",
    "    # \"Linear SVM\" : SVC(kernel=\"linear\", C=0.025),\n",
    "    # \"Decision Tree\" : DecisionTreeClassifier(max_depth=5),\n",
    "    # \"Random Forest\" : RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    # \"Neural Net\" : MLPClassifier(alpha=1, max_iter=1000),\n",
    "    \"Naive Bayes\" : GaussianNB(),\n",
    "    # \"LDA\" : LinearDiscriminantAnalysis(),\n",
    "}\n",
    "\n",
    "\n",
    "for name, algorithm in algorithms.items():\n",
    "    algorithm.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    score = algorithm.score(X_test, y_test)\n",
    "    print(name, round(score,2))\n",
    "\n",
    "    #Medell speichern\n",
    "import joblib\n",
    "joblib.dump(algorithms[\"Naive Bayes\"], 'bitcoin_Naive_Bayes.joblib')\n",
    "best_model = joblib.load('bitcoin_Naive_Bayes.joblib')\n",
    "best_model\n",
    "\n",
    "leo = pd.DataFrame([[61868.81, 61888.18, 61919.98, 2.94607889]], columns=X_train.columns)\n",
    "leo.head()\n",
    "\n",
    "Sahra = pd.DataFrame([[61882.74, 61903.16, 61882.74, 2.94235679]], columns=X_train.columns)\n",
    "Sahra.head()\n",
    "\n",
    "Michelle = pd.DataFrame([[61854.82, 61854.82, 61903.15, 3.0478475]], columns=X_train.columns)\n",
    "Michelle.head()\n",
    "\n",
    "predictions1 = best_model.predict(pd.concat([leo]))\n",
    "for person, prediction1 in zip([\"Leo\"], predictions1):\n",
    "    print(\"Erwartet: 61920, Berechnet:\", prediction1)\n",
    "\n",
    "predictions2 = best_model.predict(pd.concat([Sahra]))\n",
    "for person, prediction2 in zip([\"Sahra\"], predictions2):\n",
    "    print(\"Erwartet: 61912.67, Berechnet:\", prediction2)\n",
    "\n",
    "predictions3 = best_model.predict(pd.concat([Michelle]))\n",
    "for person, prediction3 in zip([\"Michelle\"], predictions3):\n",
    "    print(\"Erwartet: 61907.96, Berechnet:\", prediction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "predictions1 = best_model.predict(X_test)\n",
    "\n",
    "print(classification_report(predictions1, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Wählen Sie geeignete Bedingungen und erstellen Sie eine Wahrheitsmatrix für Ihr Modell. Berechnen Sie darüber hinaus Sensitivität und Spezifizität."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a logistic regression model on the training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# make class predictions for the testing set\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "# examine the class distribution of the testing set (using a Pandas Series method)\n",
    "y_test.value_counts()\n",
    "\n",
    "# calculate the percentage of ones\n",
    "# because y_test only contains ones and zeros, we can simply calculate the mean = percentage of ones\n",
    "y_test.mean()\n",
    "\n",
    "# calculate the percentage of zeros\n",
    "1 - y_test.mean()\n",
    "\n",
    "# calculate null accuracy in a single line of code\n",
    "# only for binary classification problems coded as 0/1\n",
    "max(y_test.mean(), 1 - y_test.mean())\n",
    "\n",
    "# calculate null accuracy (for multi-class classification problems)\n",
    "y_test.value_counts().head(1) / len(y_test)\n",
    "\n",
    "# print the first 25 true and predicted responses\n",
    "print('True:', y_test.values[0:25])\n",
    "print('False:', y_pred_class[0:25])\n",
    "\n",
    "# IMPORTANT: first argument is true values, second argument is predicted values\n",
    "# this produces a 2x2 numpy array (matrix)\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))\n",
    "\n",
    "# print the first 25 true and predicted responses\n",
    "print('True', y_test.values[0:25])\n",
    "print('Pred', y_pred_class[0:25])\n",
    "\n",
    "# save confusion matrix and slice into four pieces\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(confusion)\n",
    "#[row, column]\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "\n",
    "# use float to perform true division, not integer division\n",
    "print((TP + TN) / float(TP + TN + FP + FN))\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print(classification_error)\n",
    "print(1 - metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "sensitivity = TP / float(FN + TP)\n",
    "\n",
    "print(sensitivity)\n",
    "print(metrics.recall_score(y_test, y_pred_class))\n",
    "\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(specificity)\n",
    "\n",
    "false_positive_rate = FP / float(TN + FP)\n",
    "\n",
    "print(false_positive_rate)\n",
    "print(1 - specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazit: Für Dieses Datenset ist es nicht möglich die Teilaufgabe 4.3 durchzuführen. Es wird dafür eine Spalte in y benötigt, die nur aus Nullen und Einsen besteht. Oder das man eine Spalte umwandelt. Dies ist allerdings nicht möglich, da mein Datenset keine Spalten hat, wo es logisch wäre die spalten in Nullen und Einsen zu konvertieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Fassen Sie in 50 bis 100 Wörtern zusammen, wie gut Ihr Modell funktioniert, und stellen Sie Hypothesen auf, warum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für Die Aufgaben 1, 2 und 3 ist mein Datenset sehr gut geeignet. Dies liegt wahrscheindlich daran, dasss die Spalten alle einen Zusammenhang haben und nur aus Zahlen bestehen. Die einzige Schwierigkeit ist die Spalte \"time\". Diese habe ich bei der Berechnung einfach nicht beachtet.\n",
    "Für die Teilaufgabe 4 ist mein Datenset nicht so geeignet. Denn diese erfordert eine Spalte in y, die nur aus Nullen und Einsen besteht. Dies war allerdings nicht möglich."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6290412f41753d2501b5150002034550761936216b42d4e1ddd66fe2a0ab9705"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
